
# Introduction #  
  
This documentation will attempt to document the technical side of Transcriptor frontend. Transcriptor allows user to view and edit transcritions, with some quality of life features like waveform and audio playback control. In order to develop and maintain Transcriptor, you would need the following minimum technical knowledge:  
- Javascript  
- React  
- Redux

Click on the following links to jump to the part that you are interested.

 - [Redux states](#redux-states)
 - [Routes and authentication](#routes-and-authentication)
	 - [Routes](#routes)
	 - [Authentication](#authentication)
 - [Transcriptions List Component](#transcriptions-list-component)
 - [Editor](#editor)
	 - [Audio Playback](#audio-playback)
	 - [Transcript List](#transcript-list)
	 - [Waveform](#waveform) 
	 - [Block](#block)

# Redux States and Middlewares # 
Transcriptor uses [Redux](https://redux.js.org/introduction/getting-started) to handle its global states. Transcriptor's states are divided into 2 main categories:

 - User states
 - Transcriptor states

As the name suggests, user states will handle user's login information, while transcriptor states will handle states needed in order for the transcriptor components to behave accordingly, each has their own reducer and actions for easy maintaining and debugging.

Additionally, Transcriptor uses a custom middleware for socket communication, and [Thunk](https://github.com/reduxjs/redux-thunk) to handle async actions.

If you are new to Redux and React as a whole, it is strongly recommended to touch up on basic about Redux states, actions, dispatcher, reducer, and middlewares. 

# Routes and Authentication #

## Routes ##
Transcriptor uses [React Router](https://reactrouter.com/web/guides/quick-start) to handle app routes. Routes are handled in the frontend itself, indicated by the use of *Hash Router*. Routes are distinguished between 2 categories, *public routes* and *private routes*. If the user is currently signed in, they can only access private routes, and vice versa. This is to prevent unauthorized access to the Transcriptor fearures and data. 

User's log in status is indicated by the `user` state in User states. `Null` object indicates no log in, and vice versa.

**App.js**
```javascript
const  PrivateRoute = () => {
	const { user } = useSelector(state  => ({ ...state.USER }))
	return !user ? (
		<Redirect  to="login"  />
	) : (
	<Switch>
		<Route  path="/dashboard"  exact={true}  component={Dashboard}  />
		<Redirect  from="/"  to="/dashboard"  />
	</Switch>
	)
}
```
Transcriptor uses [JWT token](https://jwt.io/) generated by the backend for login persistence, stored in *localStorage* with the key *token*. Each time the user access the app, the top level component will request for token authorization via the function `requestSocketAuthentication`, if one exists. If the token is authorized, the socket will respond with the user info, and it will be placed in the user state. If token authorization fails, no action will be taken, the `user` state will remain `null`, and the user will be taken to the `login` page.

**middleware/sockets/index.js**
```javascript
const  authenticateSocket = _token  => {
	const  token = _token || localStorage.getItem('token')
	if (token) {
		store.dispatch(setIsAuthenticatingSocket(true))
		socket.emit('authenticate', {
			token,
		})
	}
}

socket.on('authenticated', user  => {
	store.dispatch(socketConnectionAuthenticated())
	store.dispatch(setUserAuthed(user))
	store.dispatch(setIsAuthenticatingSocket(false))
	socket.emit('join room')
})

socket.on('authentication failed', data  => {
	store.dispatch(setIsAuthenticatingSocket(false))
})
```

**Note**: Due to the nature of redux state,  the `user` state will start off as `null`, making the app flash the login page everytime the app is accessed, even though a valid token is present. Therefore, the state `isAuthenticatingToken` is used to prevent that. If it is `true`, the login component will return `null`.

## Authentication ##
Transcriptor's authentication is taken care by the `LoginPage` and `RegisterPage` component. Upon entering the app for the first time, or the token present is not valid, the user will be taken to the Login Page. Then the login process will be taken care by the function `signUserIn`, where the email and password will be verified in the backend. If valid, the backend will return a toke, then the function `requestSocketAuthentication` will be reused with said token.

If however, the user creates a new credential, the function `registerUser` will be used, where the given information will be sent to the backend. Upon completion, `registerUser` will call `signUserIn`, thus signing the user in.

Logging out involves resetting the user state to null, clearing *localStorage*, and emitting an event to the socket. Check `signUserOut` function.

**Note**: All the above mentioned functions (`signUserIn`, `registerUser`, `signUserOut` can be found in **UserAction.js**)

# Transcriptions List Component #
The `ListTranscriptions` component will display the transcriptions that the user has access to. The component uses [Material UI Table](https://material-ui.com/components/tables/), with server-side pagination. The limit as well as the page number will be sent to the backend, and the list will render the responded data. Each transcription will be render by the `CustomTranscriptCard` component. The transcription list will be fetched by the below `useEffect`:

**ListTranscription.js**
```javascript
useEffect(() => {
	if (subPage.toLowerCase() === 'created') {
		dispatch(
			getTranscriptionList(currentTranscriptListPage, transcriptListLimit),
		)
	} else  if (subPage.toLowerCase() === 'assigned') {
		dispatch(
			getAssignedTranscriptionList(
				currentTranscriptListPage,
				transcriptListLimit,
				false,
			),
		)
	}
}, [dispatch, currentTranscriptListPage, transcriptListLimit, subPage])
```
# Editor #
The `Editor` is the main feature of the Transcriptor, as well as being the most complicated. There are 3 main sub-components to the editor, which are:

 - Audio Playback
 - Transcript list
 - Waveform

The Editor uses the state `transcriptionId` to distinguish between the transcripts, and fetch the necessary data from the backend using the action `loadAudioSource`. The audio blob will then be decoded into [AudioBuffer](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer) (why we need to do this will be covered in the Waveform section), and an `HTMLAudioElement` instance will be created (the purpose of which will be covered in the [Audio Playback](#audio-playback) section). All the relevant data will then be dispatched into the states.

**Note**: All the relevant API like `HTMLAudioElemenet` object or `wavesurfer` object after creation will be dispatched and stored inside the redux states, so that it can be used easily across multiple components.

## Audio Playback ##
The audio playback is handled and manipulated using the default javascript [HTMLAudioElement](https://developer.mozilla.org/en-US/docs/Web/API/HTMLAudioElement). The audio controls are rendered on screen, and will call the appropriate HTMLAudioElement's method and property. The audio playback then needs to synced between the audio timeline and pointer, the current selected transcript in the transcripts list, and the timeline and pointer in the waveform. This is achieved using a series of `useEffect` hook. Also noted that when we modify the audio pointer using any methods (either by the playback timeline or waveform timeline or clicking on a transcript), we need to sync between all 3 of them also.

**Note**:  Be mindful of the range of value on different components (e.g. The range input of Material UI is from 0-100, where as Wavesurfer's waveform is from 0-1)
## Transcript List ##
The transcript list is rendered by the `Annotation` component and it is where the user can edit view and edit the transcripts. For performance reason, the list is rendered using [react-virtualized table](https://github.com/bvaughn/react-virtualized/blob/master/docs/Table.md). Each row is built using a separate component `TranscriptRow`, which whenever changed will call the `update` prop, which will in turn dispatch an `updateTranscripts` action. Noted that each `TranscriptRow` has its own state, and the update is debounced. This helps improve performance and resposiveness of when updating. Also note when updating a sentence (not creating or deleting), the backend api call is also debounced. This is to prevent multiple wasting api calls. When creating or deleting there is no need to debounced since each time will be a new operation.

When the audio is playing, a row will be highlighted based on the current audio pointer. This will be synced with the corresponding `Block` on the waveform (will be discussed later). Clicking on a block will also seek the audio pointer to the start of that sentence.

The transcripts list data is kept in the `transcripts` state of Transcription State. Each time there is a change in the list, a new list will be created and dispatched.

**Note**: Notice that in the `rowRenderer` function of the react-virtualized `Table`, we don't use the key passed in by the component like recommended but rather the sentenceId. This is because that key is just the order of the element in the list, and thus react can't distingush between them whenever the order might change (a new sentence is added or old sentence deleted).

## Waveform ##
Waveform is rendered using the [wavesurfer.js](https://wavesurfer-js.org/) library, along with 2 more plugins for [regions](https://wavesurfer-js.org/plugins/regions.html) and [timeline](https://wavesurfer-js.org/plugins/timeline.html). 

The audio blob will be decoded into `AudioBuffer` before being input into `wavesurfer` rather than just input the blob directly. The reason is twofold:

 - `AudioBuffer` is needed for rendering single channel waveform: Wavesurfer does not have built in channel selector for waveform. However, we can manually extract each channel data from an `AudioBuffer` into multiple `AudioBuffer`, and use it to input into `wavesurfer`. This can be achieved by the following block of code. For full implementation see `getChannelAudioBuffersFromABlob` and `getChannelAudioBuffersFromAnAudioBuffer`
	```javascript
	const  audioBuffers = []
	const  numOfChannels = data.numberOfChannels
	
	for (let  i = 0; i < numOfChannels; i++) {
		const  currentChannel = data.getChannelData(i) 
		const  newChannelArrayBuffer = ac.createBuffer(1, data.length, 44100) 
		newChannelArrayBuffer.copyToChannel(currentChannel, 0)
		audioBuffers.push(newChannelArrayBuffer)
	}
	```

 - The waveform only renders a part of the audio at a time rather than all. This is to improve visibility and performance. The start and end of the section will be determined based on the current audio pointer as well as the portion duration selected. We cannot achieve this if we use a blob, however, with an `AudioBuffer`, we can calculate the start and end index of the audio data from the start, end time and the sample rate of the buffer and extract the part of the buffer needed. For full implementation see `getSectionAudioBuffer`
	```javascript
	const { numberOfChannels, sampleRate } = audioBuffer
	const  newArrayBuffer = audioCtx.createBuffer(
		numberOfChannels,
		(end - start) * sampleRate,
		sampleRate,
	) 
	
	const  startIndex = clamp(start * sampleRate, 0, Infinity)
	const  endIndex = clamp(end * sampleRate, startIndex, Infinity) 
	 
	for (let  i = 0; i < numberOfChannels; i++) {
		const  dataToCopy = audioBuffer.getChannelData(i)
		const  newChannelData = newArrayBuffer.getChannelData(i) 
			for (let  j = startIndex; j < endIndex; j++) {
				newChannelData[j - startIndex] = dataToCopy[j]
			}
	}
	```

As mentioned above, the waveform `onClick` event will seek the audio playback and sync it across all components.
```javascript
wavesurfer.drawer.on('click', (e, progress) => {
	if (progress >= 0) {
		onSeek(progress)
	}
})
```

**Note**: Although the waveform is only rendering one part of the audio, from the wavesurfer methods, the start time and end time is still from 0 to 1. We need to calculate the offset from the current section to the actual start and end time of the audio. One example
```javascript
const  waveSurferPercentage = (_currentTime - sectionToGoTo * sectionLength) / sectionLength

wavesurfer.seekTo(waveSurferPercentage)
```

## Block ##
`Block` element is the representation of the sentences on the waveform. Each block will accurately represents the position as well as the duration with regards to the audio. In order to achieve that we need to calculate the `sec/pixel` multiplier for the current section, then we can multiply it with the sentence length for the width of each block (note that the multiplier can change due to either the section length or window width).

```javascript
const  measuredMultiplier = useCallback( el  => {
	setMultiplier(el && sectionLength ? el.clientWidth / sectionLength : 0)
}, [sectionLength])
```

Since the waveform only renders a section of the audio, only parts of the whole transcripts will be rendered. As the start and end time of the section changes, so will the sentences.
```javascript
useEffect(() => {
	const  currentNotes = filter(
	map(notes, (d, index) => ({ ...d, rowIndex:  index })),
	({ begin, end }) =>
		(begin >= startSection && begin <= endSection) || // begin time between section
		(end >= startSection && end <= endSection) || // end time between section
		(begin < startSection && end > endSection), // the whole transcript contain the section
	)
	setCurrentTranscripts(currentNotes)
}, [notes, endSection, startSection])
```
`Block` allows the user to drag and drop to change the position of the sentences with regards to the waveform, or drag only start or end time. This is achieved by leveraging the `translate` property of css. The detailed flow is as followed:

 1. Each block will have 3 regions: `left` to expand the sentence to the left, `right` to expand the sentence to the right, or `center` to move the whole sentence. Each will have an `onMouseDown` listener, which when triggered, will set the flag `isDragging` to `true` (which indicate that the user is clicking on a block and may or may not drag it), set the `lastType` of region clicked, set the ref `lastRef` of the block clicked, the current x position of the block clicked `lastX` and the current width of the block `lastWidth`, as well as some more properties.
	```javascript
	const  onMouseDown = useCallback((key, event, type) => {
		isDragging = true
		lastSub = notes[key]
		lastType = type
		lastX = event.pageX
		lastIndex = key
		lastTarget = $subsRef.current.children[lastIndex]
		lastWidth = parseFloat(lastTarget.style.width)
	}, [notes])
	```
 2. The document is also attached 2 listeners: `onMouseMove` and `onMouseUp`.
 3. When `onMouseMove` is triggered when `isDragging` is true (meaning the user is actively dragging the block), it will calculate the x offset based on `lastX`, then modify the DOM's width and translate directly using `lastRef` and `lastWidth`.
	```javascript
	const  onDocumentMouseMove = useCallback(event  => {
		if (isDragging && lastTarget && lastSub) {
			lastDiffX = event.pageX - lastX
			if (lastType === 'left') {
				lastTarget.style.width = `${lastWidth - lastDiffX}px`
				lastTarget.style.transform = `translate(${lastDiffX}px)`
			} else  if (lastType === 'right') {
				lastTarget.style.width = `${lastWidth + lastDiffX}px`
			} else {
				const  fullWidth = (parseFloat(lastSub.end) - parseFloat(lastSub.begin)) * multiplier  
				lastTarget.style.transform = `translate(${lastDiffX}px)`
				lastTarget.style.width = `${fullWidth}px`
			}
		}
	}, [multiplier])
	```

 4. Then when the mouse click is released, `onMouseUp` will clean up the `isDragging` flag (if it was true before), set it to false, as well as clean up all the properties like `lastType` or `lastX`, while calling the update function to update the new start and end time of the sentence if any. Note that it will check the validity of the new start and end time first. Only if valid then it would call the update, else it would revert the sentence back to the old width and x position.
	```javascript
	const  onDocumentMouseUp = useCallback(() => {
		if (isDragging && lastTarget && lastDiffX) {
			const  timeDiff = lastDiffX / multiplier
			const  previous = notes[lastIndex - 1]
			const  next = notes[lastIndex + 1]
			
			const  startTime = magnetically(
				parseFloat(lastSub.begin) + timeDiff,
				previous ? parseFloat(previous.end) : 0,
			)
			const  endTime = magnetically(
				parseFloat(lastSub.end) + timeDiff,
				next ? parseFloat(next.begin) : 0,
			)
			const  width = (Math.min(endSection, endTime) - Math.max(startSection, startTime)) * multiplier
			
			if (lastType === 'left') {
				if (startTime >= 0 && startTime < lastSub.end) {
					updateNote(notes[lastIndex].sentenceId, { begin:  startTime })
				} else {
					lastTarget.style.width = `${width}px`
				}
			} else  if (lastType === 'right') {
				if (endTime >= 0 && endTime > lastSub.begin) {
					updateNote(notes[lastIndex].sentenceId, { end:  endTime })
				} else {
					lastTarget.style.width = `${width}px`
				}
			} else {
				if (startTime > 0 && endTime > 0 && endTime > startTime) {
					lastTarget.style.width = `${width}px`
					updateNote(notes[lastIndex].sentenceId, {
						begin:  startTime,
						end:  endTime,
					})
				} else {
					lastTarget.style.width = `${width}px`
				}
			}
			lastTarget.style.transform = 'translate(0)'
		}
		lastTarget = null
		lastType = ''
		lastX = 0
		lastWidth = 0
		lastDiffX = 0
		isDragging = false
		lastSub = null
	}, [multiplier, notes, updateNote, endSection, startSection])
	```
